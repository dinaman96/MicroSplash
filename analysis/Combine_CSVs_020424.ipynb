{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenario: A for slice A1_300623\n",
      "Processing scenario: B for slice A1_300623\n",
      "Processing scenario: A for slice A2_250623\n",
      "Processing scenario: B for slice A2_250623\n",
      "Processing scenario: A for slice A2_260623\n",
      "Processing scenario: B for slice A2_260623\n",
      "Processing scenario: A for slice B1_300623\n",
      "Processing scenario: B for slice B1_300623\n",
      "Processing scenario: A for slice B2_020723\n",
      "Processing scenario: B for slice B2_020723\n",
      "Processing scenario: A for slice B2_200623\n",
      "Processing scenario: B for slice B2_200623\n",
      "Processing scenario: A for slice B2_250623\n",
      "Processing scenario: B for slice B2_250623\n",
      "Processing scenario: A for slice B2_260623\n",
      "Processing scenario: B for slice B2_260623\n",
      "Processing scenario: A for slice B2_300623\n",
      "Processing scenario: B for slice B2_300623\n",
      "Processing scenario: A for slice B3_100423\n",
      "Processing scenario: B for slice B3_100423\n",
      "Processing scenario: A for slice C3_100423\n",
      "Processing scenario: B for slice C3_100423\n",
      "          Slice  Count  Droplet        Area  time Well    date Scenario\n",
      "0     A1_300623      0        1     124.473     0   A1  300623        A\n",
      "1     A1_300623      0        2    6512.656     0   A1  300623        A\n",
      "2     A1_300623      0        3     770.468     0   A1  300623        A\n",
      "3     A1_300623     58        4  169471.922     0   A1  300623        A\n",
      "4     A1_300623      0        5     747.925     0   A1  300623        A\n",
      "...         ...    ...      ...         ...   ...  ...     ...      ...\n",
      "2588  C3_100423      0     2589     705.019    24   C3  100423        B\n",
      "2589  C3_100423      0     2590    1324.769    24   C3  100423        B\n",
      "2590  C3_100423      0     2591     642.401    24   C3  100423        B\n",
      "2591  C3_100423      0     2592     455.529    24   C3  100423        B\n",
      "2592  C3_100423      0     2593    1205.523    24   C3  100423        B\n",
      "\n",
      "[990150 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "def combine_csvs_with_roi_and_drop_errors(count_folder, roi_folder, errors_file=None):\n",
    "    slice_folders = glob.glob(os.path.join(count_folder, '*'))\n",
    "    results = []\n",
    "    \n",
    "    for slice_folder in slice_folders:\n",
    "        slice_name = os.path.basename(slice_folder)\n",
    "        roi_file = os.path.join(roi_folder, f'{slice_name}.csv')\n",
    "        \n",
    "        if not os.path.exists(roi_file):\n",
    "            print(f\"ROI file for slice {slice_name} is missing.\")\n",
    "            continue\n",
    "            \n",
    "        roi_table = pd.read_csv(roi_file)\n",
    "        roi_table.columns = ['Droplet', 'Area']\n",
    "        \n",
    "        scenario_folders = glob.glob(os.path.join(slice_folder, '*'))\n",
    "        \n",
    "        for scenario_folder in scenario_folders:\n",
    "            scenario_name = os.path.basename(scenario_folder)\n",
    "            count_files = glob.glob(os.path.join(scenario_folder, '*.csv'))\n",
    "            \n",
    "            if not count_files:\n",
    "                print(f\"Skipping scenario {scenario_name} for slice {slice_name} as count files are missing.\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Processing scenario: {scenario_name} for slice {slice_name}\")\n",
    "            \n",
    "            for count_file in count_files:\n",
    "                csv_name = os.path.basename(count_file).replace(\".csv\", \"\")\n",
    "                temp = pd.read_csv(count_file)\n",
    "                temp['Scenario'] = scenario_name  # Add scenario information\n",
    "                combined = pd.concat([temp, roi_table], axis=1)\n",
    "                combined['Slice'] = csv_name  # Store the name of the CSV file under 'Slice'\n",
    "                results.append(combined)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No data to process.\")\n",
    "        return None\n",
    "    \n",
    "    all_results = pd.concat(results)\n",
    "    all_results = all_results.drop(columns=['Total Area', 'Average Size', '%Area', 'Perim.'])\n",
    "    all_results['Slice'] = all_results['Slice'].str.replace(\"_Simple Segmentation\", \"\")\n",
    "    all_results[['time', 'Well', 'date']] = all_results['Slice'].str.split('_', n=2, expand=True)\n",
    "    all_results['time'] = all_results['time'].str.replace(\"h\", \"\").astype(int)\n",
    "    all_results['Slice'] = all_results['Slice'].str.replace(r'\\d+h_', '', regex=True)\n",
    "    all_results = all_results[['Slice', 'Count', 'Droplet', 'Area', 'time', 'Well', 'date', 'Scenario']]\n",
    "\n",
    "    if errors_file is not None:\n",
    "        # Read the error Excel file\n",
    "        droplets_to_remove = pd.read_excel(errors_file)\n",
    "\n",
    "        # Process each column (slice) in the Excel file\n",
    "        for slice_name in droplets_to_remove.columns:\n",
    "            # Get the list of droplets to remove for this slice\n",
    "            droplets_to_drop = droplets_to_remove[slice_name].dropna().astype(int).tolist()\n",
    "\n",
    "            # Remove these droplets from the all_results DataFrame for the specific slice\n",
    "            all_results = all_results.loc[~((all_results['Droplet'].isin(droplets_to_drop)) & (all_results['Slice'] == slice_name)), :]\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "count_folder = 'C:\\\\Users\\\\dinam\\\\Documents\\\\master\\\\Experiments\\\\New analysis\\\\Results_files\\\\Count'\n",
    "roi_folder = 'C:\\\\Users\\\\dinam\\\\Documents\\\\master\\\\Experiments\\\\New analysis\\\\Results_files\\\\ROIcsv'\n",
    "errors_file = 'C:\\\\Users\\\\dinam\\\\Documents\\\\master\\\\Experiments\\\\New analysis\\\\Results_files\\\\droplets_toRemove.xlsx'\n",
    "\n",
    "combined_df = combine_csvs_with_roi_and_drop_errors(count_folder, roi_folder, errors_file)\n",
    "print(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ** 19655 droplets after filtering\n"
     ]
    }
   ],
   "source": [
    "df = combined_df.copy()\n",
    "df['DW'] = df['Droplet'].astype(str) + '_' + df['Slice'].astype(str)\n",
    "\n",
    "def update_droplet_counts(df):\n",
    "    # Group by 'DW' and 'Scenario', then calculate the sum of counts and any/all zeros condition\n",
    "    group = df.groupby(['DW', 'Scenario'])\n",
    "    sum_counts = group['Count'].transform('sum')\n",
    "    any_zeros = group['Count'].transform(lambda x: any(x == 0))\n",
    "    all_zeros = group['Count'].transform(lambda x: all(x == 0))\n",
    "\n",
    "    # Identify the droplets with sum counts < 25 and not all zeros\n",
    "    update_condition = (sum_counts < 25) & ~all_zeros\n",
    "    \n",
    "    # Find 'DW's where any scenario meets the update condition\n",
    "    dws_to_update = df.loc[update_condition, 'DW'].unique()\n",
    "\n",
    "    # Update counts to 0 for both scenarios in these 'DW's\n",
    "    df.loc[df['DW'].isin(dws_to_update), 'Count'] = 0\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_irregular_droplets(df):\n",
    "    # Count the number of zeros in Weighted_Count for each DW\n",
    "    zero_counts = df.groupby('DW')['Weighted_Count'].transform(lambda x: (x == 0).sum())\n",
    "\n",
    "    # Identify DWs with more than 4 but not all zeros in Weighted_Count\n",
    "    more_than_5_zeros = zero_counts > 4\n",
    "    all_zeros = df.groupby('DW')['Weighted_Count'].transform(lambda x: all(x == 0))\n",
    "    to_remove = more_than_5_zeros & ~all_zeros\n",
    "\n",
    "    # Filter out the droplets that need to be removed\n",
    "    filtered_df = df[~to_remove]\n",
    "\n",
    "    # Create a DataFrame of removed droplets\n",
    "    removed_droplets = df[to_remove].drop_duplicates()\n",
    "\n",
    "    return filtered_df, removed_droplets\n",
    "\n",
    "\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "filtered_df = update_droplet_counts(df.copy())\n",
    "\n",
    "\n",
    "# Vectorized time delay calculation with .loc\n",
    "time_delay_dict = {'A1': 0, 'A2': 15/60, 'B2': 45/60, 'B1': 30/60, 'B3': 0, 'C3': 27/60}\n",
    "filtered_df.loc[:, 'actual_time'] = filtered_df['time'] + filtered_df['Well'].map(time_delay_dict)\n",
    "\n",
    "# Vectorized Area_to_Volume calculation with .loc\n",
    "Theta = np.radians(32)\n",
    "D = 2 * np.sqrt(filtered_df['Area'] / np.pi)\n",
    "filtered_df.loc[:, 'Volume'] = ((np.pi * D**3) / 24) * ((2 - 3 * np.cos(Theta) + np.cos(Theta)**3) / (np.sin(Theta)**3))\n",
    "filtered_df.loc[:, 'log_Volume'] = np.log10(filtered_df['Volume'])\n",
    "\n",
    "# Vectorized condition for InitialOD with .loc\n",
    "condition_a = filtered_df['Well'].str.startswith('A')\n",
    "condition_b3_c3 = filtered_df['Well'].isin(['B3', 'C3'])\n",
    "filtered_df.loc[:, 'InitialOD'] = np.where(condition_a | condition_b3_c3, '0.01', '0.03')\n",
    "\n",
    "# Binning with .loc\n",
    "vol_labels = ['3 - 4', '4 - 5', '5 - 6', '6 - 7', '7 - 8']\n",
    "cut_bins_vol = [3, 4, 5, 6, 7, 8]\n",
    "filtered_df.loc[:, 'Bins_vol'] = pd.cut(filtered_df['log_Volume'], bins=cut_bins_vol)\n",
    "filtered_df.loc[:, 'Bins_vol_txt'] = pd.cut(filtered_df['log_Volume'], bins=cut_bins_vol, labels=vol_labels)\n",
    "\n",
    "\n",
    "# Define Gaussian functions for each scenario\n",
    "def right_tail_gaussian(x, mu=0, sigma=4):\n",
    "    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))\n",
    "\n",
    "def normal_gaussian(x, mu=24, sigma=8):\n",
    "    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))\n",
    "\n",
    "# Apply Gaussian functions vectorized\n",
    "filtered_df['Weight_A'] = np.where(filtered_df['Scenario'] == 'A', right_tail_gaussian(filtered_df['time']), 0)\n",
    "filtered_df['Weight_B'] = np.where(filtered_df['Scenario'] == 'B', normal_gaussian(filtered_df['time']), 0)\n",
    "\n",
    "# Normalize weights so they sum to 1 for each droplet and time point\n",
    "group_cols = ['Slice', 'time', 'Droplet']\n",
    "total_weights = filtered_df.groupby(group_cols)[['Weight_A', 'Weight_B']].transform('sum').sum(axis=1)\n",
    "filtered_df['Weight_A'] /= total_weights\n",
    "filtered_df['Weight_B'] /= total_weights\n",
    "\n",
    "# Calculate the weighted count in each row\n",
    "filtered_df['Weighted_Count_Row'] = filtered_df['Weight_A'] * filtered_df['Count'] + filtered_df['Weight_B'] * filtered_df['Count']\n",
    "\n",
    "# Group by droplet and time to sum these up into a new DataFrame\n",
    "result_df = filtered_df.groupby(group_cols).agg({'Weighted_Count_Row': 'sum'}).reset_index()\n",
    "result_df.rename(columns={'Weighted_Count_Row': 'Weighted_Count'}, inplace=True)\n",
    "\n",
    "# Get additional columns from the original DataFrame\n",
    "additional_columns_df = filtered_df.groupby(group_cols)[['Well', 'date', 'DW', 'actual_time','Area', 'Volume', 'log_Volume', 'InitialOD', 'Bins_vol', 'Bins_vol_txt']].first().reset_index()\n",
    "\n",
    "# Merge the additional columns into result_df\n",
    "result_df = pd.merge(result_df, additional_columns_df, on=group_cols, how='left')\n",
    "\n",
    "result_df, removed_droplets = remove_irregular_droplets(result_df)\n",
    "print(f\" ** {result_df['DW'].nunique()} droplets after filtering\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by 'DW' and time \n",
    "weighted_counts_df = result_df.sort_values(by=['DW', 'time'])\n",
    "\n",
    "# Define a function to apply the condition for each droplet\n",
    "def update_counts(group):\n",
    "    # Check if the first and second time points are < 1 and the rest are > 0\n",
    "    if group['Weighted_Count'].iloc[0] < 1 and group['Weighted_Count'].iloc[1] < 1 \\\n",
    "       and all(group['Weighted_Count'].iloc[2:] > 0):\n",
    "        group['Weighted_Count'].iloc[0] = 1\n",
    "        group['Weighted_Count'].iloc[1] = 1\n",
    "    return group\n",
    "\n",
    "# Apply the function to each group of droplets\n",
    "weighted_counts_df = weighted_counts_df.groupby('DW').apply(update_counts).reset_index(drop=True)\n",
    "\n",
    "filtered_df = weighted_counts_df.copy()\n",
    "filtered_df = filtered_df.drop(['Well', 'date'], axis=1)\n",
    "filtered_df = filtered_df.loc[filtered_df['log_Volume'] >= 3]\n",
    "filtered_df['InitialOD'] = filtered_df['InitialOD'].astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OnlyBac_df(data_df):\n",
    "    # Filter out droplets that have zero cells in all time points\n",
    "    return data_df[data_df.groupby('DW')['Weighted_Count'].transform('sum') > 0]\n",
    "\n",
    "def log_mean_fill(series):\n",
    "    result = series.copy()\n",
    "    for idx in range(1, len(series)):\n",
    "        if pd.isna(series.iloc[idx]):\n",
    "            before = series.iloc[idx - 1] if idx - 1 >= 0 else np.nan\n",
    "            after = series.iloc[idx + 1] if idx + 1 < len(series) else np.nan\n",
    "\n",
    "            # If both before and after are valid numbers, use log mean\n",
    "            if pd.notna(before) and pd.notna(after) and before > 0 and after > 0:\n",
    "                result.iloc[idx] = 10 ** ((np.log10(before) + np.log10(after)) / 2)\n",
    "            else:\n",
    "                # Find the nearest valid previous value for each NaN\n",
    "                found_valid_value = False\n",
    "                for j in range(1, idx + 1):\n",
    "                    prev_value = series.iloc[idx - j] if idx - j >= 0 else np.nan\n",
    "                    if pd.notna(prev_value) and prev_value > 0:\n",
    "                        result.iloc[idx] = prev_value\n",
    "                        found_valid_value = True\n",
    "                        break\n",
    "    return result\n",
    "\n",
    "def process_droplet(group):\n",
    "    group['Percent_Change'] = group['Weighted_Count'].pct_change()\n",
    "    decrease_threshold = -0.5  # Threshold for decrease\n",
    "    group['Weighted_Count'].where(~(group['Percent_Change'] < decrease_threshold), np.nan, inplace=True)\n",
    "    group['Weighted_Count'] = log_mean_fill(group['Weighted_Count'])\n",
    "    return group\n",
    "\n",
    "\n",
    "def apply_lowess(group):\n",
    "    counts = group['Weighted_Count'].values\n",
    "    if len(counts) > 1:\n",
    "        log_counts = np.log10(counts + 1)  # Adding 1 to avoid log(0)\n",
    "        smoothed_log_values = lowess(log_counts, np.arange(len(log_counts)), frac=0.2)[:, 1]\n",
    "        smoothed_values = 10**smoothed_log_values - 1  # Subtracting 1 to revert the earlier addition\n",
    "        return pd.Series(smoothed_values, index=group.index)\n",
    "    else:\n",
    "        return pd.Series([np.nan] * len(group), index=group.index)\n",
    "    \n",
    "slice_to_new_name = {\n",
    "    'A1_300623': 'C1LD',\n",
    "    'B3_100423': 'C2LD',\n",
    "    'C3_100423': 'C3LD',\n",
    "    'A2_260623': 'C4LD',\n",
    "    'A2_250623': 'C5LD',\n",
    "    'B1_300623': 'C6HD',\n",
    "    'B2_020723': 'C7HD',\n",
    "    'B2_300623': 'C8HD',\n",
    "    'B2_260623': 'C9HD',\n",
    "    'B2_200623': 'C10HD',\n",
    "    'B2_250623': 'C11HD'}\n",
    "\n",
    "filtered_df['Slice'] = filtered_df['Slice'].map(slice_to_new_name)\n",
    "filtered_df.to_csv('filtered_df.csv')\n",
    "\n",
    "cb = OnlyBac_df(filtered_df)\n",
    "cb._is_copy = None\n",
    "cb['Weighted_Count'] = cb['Weighted_Count'].apply(lambda x: max(x, 1) if pd.notna(x) else x)\n",
    "cb = cb.groupby('DW').apply(process_droplet).reset_index(drop=True)\n",
    "# Apply LOWESS smoothing to each group\n",
    "cb['lowess_count'] = cb.groupby('DW').apply(apply_lowess).reset_index(level=0, drop=True)\n",
    "\n",
    "# Merge carrying capacity and initial cell count\n",
    "cc = cb[cb['time'] >= 21].groupby('DW')['Weighted_Count'].mean().rename('Carrying_Capacity')\n",
    "ic = cb[cb['time'] == 0].groupby('DW')['Weighted_Count'].first().rename('initial_cell_count')\n",
    "cb = cb.merge(cc, on='DW', how='left').merge(ic, on='DW', how='left')\n",
    "cb['Divisions'] = np.log2(cb['Carrying_Capacity'] / cb['initial_cell_count'])\n",
    "cb['Divisions'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "cb['normalized_count'] = cb['lowess_count'] / cb['Carrying_Capacity']\n",
    "cb.to_csv('cb.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
